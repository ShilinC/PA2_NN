{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import sys\n",
    "sys.path.append('./python-mnist/')\n",
    "from mnist import MNIST\n",
    "\n",
    "def ReLU(x):\n",
    "    return np.maximum(x, 0)\n",
    "\n",
    "def sigmoid(x):\n",
    "    return 1.7159*np.tanh(2*x/3)\n",
    "\n",
    "def dReLU(x):\n",
    "    x[x >0] = 1\n",
    "    x[x <= 0] = 0\n",
    "    return x\n",
    "\n",
    "def dsigmoid(x):\n",
    "    return 1.7159*(1-np.power(np.tanh(2*x/3),2))*2/3\n",
    "\n",
    "def softmax(x):\n",
    "    # Find the largest a, and subtract it from each a in order to prevent overflow\n",
    "    x_max = np.max(x,1).reshape(x.shape[0],1)\n",
    "    sum_exp_x = np.sum(np.exp(x - x_max),1).reshape(x.shape[0],1) \n",
    "    pred_y = np.exp(x - x_max) / (sum_exp_x+0.0) \n",
    "\n",
    "    return pred_y\n",
    "\n",
    "def random_init_weights(input_size, output_size):\n",
    "    return np.random.normal(0,np.power(input_size,-0.5),(input_size,output_size))\n",
    "\n",
    "def random_init_bias(output_size):\n",
    "    return np.random.randn(1, output_size)\n",
    "\n",
    "def zero_init_delta_w(input_size, output_size):\n",
    "    return np.zeros((input_size,output_size))\n",
    "\n",
    "class Network():\n",
    "\n",
    "    def __init__(self, layers, init_method_weights = random_init_weights, init_method_bias = random_init_bias, init_method_delta_w = zero_init_delta_w, activation_fn = \"ReLU\", learning_rate = 0.01, momentum = 0.9, epoches = 30, batch_size = 256):\n",
    "        self.layers = layers\n",
    "        self.init_method_weights = init_method_weights\n",
    "        self.init_method_bias = init_method_bias\n",
    "        self.init_method_delta_w = init_method_delta_w\n",
    "\n",
    "        self.setup_layers()\n",
    "        self.epoches = epoches\n",
    "        self.learning_rate = learning_rate\n",
    "        self.momentum = momentum\n",
    "        self.batch_size = batch_size\n",
    "\n",
    "        if activation_fn == \"sigmoid\":\n",
    "            self.activation_fn = sigmoid\n",
    "            self.activation_dfn = dsigmoid\n",
    "        elif activation_fn == \"ReLU\":\n",
    "            self.activation_fn = ReLU\n",
    "            self.activation_dfn = dReLU\n",
    "\n",
    "\n",
    "    def setup_layers(self):\n",
    "        self.w = [ self.init_method_weights(input_size, output_size) for input_size, output_size in zip(self.layers[:-1], self.layers[1:])]\n",
    "        self.delta_w = [ self.init_method_delta_w(input_size, output_size) for input_size, output_size in zip(self.layers[:-1], self.layers[1:])]\n",
    "        self.b = [ self.init_method_bias(output_size) for output_size in self.layers[1:]]\n",
    "\n",
    "    def forward(self, x):\n",
    "        for weight, bias in zip(self.w[:-1], self.b[:-1]):\n",
    "            x = self.activation_fn(np.matmul(x, weight) + bias)\n",
    "\n",
    "        pred_y = softmax(np.matmul(x, self.w[-1]) + self.b[-1])\n",
    "        return pred_y\n",
    "\n",
    "    def get_activations(self, x):\n",
    "        activations = [x] \n",
    "        pre_activations = []\n",
    "        for weight, bias in zip(self.w[:-1], self.b[:-1]):\n",
    "\n",
    "            x = np.matmul(x, weight) + bias\n",
    "            pre_activations.append(x)\n",
    "            x = self.activation_fn(x)\n",
    "            activations.append(x)\n",
    "        \n",
    "        pred_y = np.matmul(x, self.w[-1]) + self.b[-1]    \n",
    "        pre_activations.append(pred_y)    \n",
    "        pred_y = softmax(pred_y)\n",
    "        activations.append(pred_y)\n",
    "        \n",
    "        return activations, pre_activations\n",
    "    \n",
    "    def gradient_check(self, dw, train_data_batch, train_label_batch, one_hot_train_label):\n",
    "        epsilon_check = np.power(10,-2)\n",
    "        for w_check, dw_check in zip(self.w, dw):\n",
    "            loss_plus = loss(train_data_batch, one_hot_train_label, train_label_batch, w_check, epsilon_check, flag = \"plus\")\n",
    "            loss_minus = loss(train_data_batch, one_hot_train_label, train_label_batch, w_check, epsilon_check, flag = \"minus\")\n",
    "            if np.abs((loss_plus - loss_minus) / (2 * epsilon_check) - dw_check) <= np.power(10,-4):\n",
    "                print(\"gradient check passed!\")\n",
    "            else\n",
    "                print(\"gradient check failed!\")\n",
    "\n",
    "    def momentum_update(self, gradient, delta_w_):\n",
    "        delta_w_ = [delta_w * self.momentum for delta_w in delta_w_]\n",
    "        delta_w_ = self.learning_rate * gradient / (self.batch_size+0.0) + delta_w_ #delta_w has same dimension as w\n",
    "        return delta_w_  \n",
    "            \n",
    "    def update_mini_batch(self, train_data_batch, train_label_batch):\n",
    "        dw = [np.zeros(weight.shape) for weight in self.w]\n",
    "        db = [np.zeros(bias.shape) for bias in self.b]\n",
    "\n",
    "        for train_data, train_label in zip(train_data_batch, train_label_batch):\n",
    "            dw_, db_ = self.backpropagation(train_data, train_label)\n",
    "            dw = [dweight + dweight_ for dweight, dweight_ in zip(dw, dw_)]\n",
    "            db = [dbias + dbias_ for dbias, dbias_ in zip(db, db_)]\n",
    "        counter = 0\n",
    "        for weight, dw_, delta_w_ in zip(self.w, dw, self.delta_w):\n",
    "            self.delta_w[counter] = self.momentum_update(dw_, delta_w_)\n",
    "            weight = weight + self.momentum_update(dw_, delta_w_)\n",
    "            self.w[counter] = weight\n",
    "            counter = counter + 1\n",
    "        #self.w = [weight + self.momentum_update(dw_, delta_w_) for weight, dw_, delta_w_ in zip(self.w, dw, self.delta_w)]\n",
    "        self.b = [bias + self.learning_rate * db_ / (train_data_batch.shape[0]+0.0)  for bias, db_ in zip(self.b, db)]\n",
    "\n",
    "\n",
    "        #self.w = [weight + self.learning_rate * dw_ / (train_data_batch.shape[0]+0.0) for weight, dw_ in zip(self.w, dw)]\n",
    "        #self.b = [bias + self.learning_rate * db_ / (train_data_batch.shape[0]+0.0)  for bias, db_ in zip(self.b, db)]\n",
    "        \n",
    "    def backpropagation(self, train_data, train_label):\n",
    "        train_data = train_data.reshape(1, train_data.shape[0])\n",
    "        dw = [np.zeros(weight.shape) for weight in self.w ]\n",
    "        db = [np.zeros(bias.shape) for bias in self.b ]\n",
    "\n",
    "        activations, pre_activations = self.get_activations(train_data)\n",
    "    \n",
    "        delta = train_label - activations[-1]\n",
    "        dw[-1] = np.matmul( activations[-2].transpose(), delta)\n",
    "\n",
    "        for idx in range(2, len(self.layers)):\n",
    "\n",
    "            pre_activation = pre_activations[-idx]\n",
    "            activation = activations[-idx-1]\n",
    "            delta = self.activation_dfn(pre_activation) * np.matmul(delta, self.w[-idx+1].transpose())\n",
    "            dw[-idx] = np.matmul( activation.transpose(), delta)\n",
    "            db[-idx] = delta  \n",
    "        return dw, db\n",
    "\n",
    "    def loss(self, input_data, one_hot_labels, labels):\n",
    "        pred_y = self.forward(input_data)\n",
    "\n",
    "        pred_y[pred_y == 0.0] = 1e-15\n",
    "        log_pred_y = np.log(pred_y)\n",
    "        loss_ = -np.sum(one_hot_labels * log_pred_y) / (one_hot_labels.shape[0]+0.0)\n",
    "\n",
    "        return loss_\n",
    " \n",
    "    def accuracy(self, input_data, one_hot_labels, labels):\n",
    "        pred_y = self.forward(input_data)\n",
    "        pred_class = np.argmax(pred_y, axis=1)\n",
    "        accuracy_ = np.sum(pred_class == labels)/(pred_class.shape[0]+0.0)\n",
    "\n",
    "        return accuracy_\n",
    "\n",
    "    def train(self, training_images, one_hot_train_labels, training_labels, test_images, one_hot_test_labels, test_labels):\n",
    "\n",
    "        self.accuracy(training_images, one_hot_train_labels, training_labels)\n",
    "\n",
    "        batch_count = training_images.shape[0] / self.batch_size\n",
    "\n",
    "        for epoch in range(self.epoches):\n",
    "            idxs = np.random.permutation(training_images.shape[0]) \n",
    "            X_random = training_images[idxs]\n",
    "            Y_random = one_hot_train_labels[idxs]\n",
    "\n",
    "            for i in range(int(batch_count)):\n",
    "                train_data_batch = X_random[i * self.batch_size: (i+1) * self.batch_size, :]\n",
    "                train_label_batch = Y_random[i * self.batch_size: (i+1) * self.batch_size, :]\n",
    "\n",
    "                self.update_mini_batch(train_data_batch, train_label_batch)\n",
    "\n",
    "            #accuracy_ = self.accuracy(training_images, one_hot_train_labels,training_labels)\n",
    "            loss_ = self.loss(training_images, one_hot_train_labels,training_labels)\n",
    "            accuracy_ = self.accuracy(test_images, one_hot_test_labels, test_labels)\n",
    "            \n",
    "            print (\"accuracy is \" + str(accuracy_))\n",
    "            \n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "accuracy is 0.9098\n",
      "accuracy is 0.9224\n",
      "accuracy is 0.9331\n",
      "accuracy is 0.9415\n",
      "accuracy is 0.9477\n",
      "accuracy is 0.9524\n",
      "accuracy is 0.9536\n",
      "accuracy is 0.957\n",
      "accuracy is 0.9607\n",
      "accuracy is 0.9572\n",
      "accuracy is 0.9619\n",
      "accuracy is 0.9624\n",
      "accuracy is 0.9634\n",
      "accuracy is 0.9631\n",
      "accuracy is 0.9641\n",
      "accuracy is 0.9648\n",
      "accuracy is 0.9666\n",
      "accuracy is 0.9662\n",
      "accuracy is 0.9681\n",
      "accuracy is 0.9675\n",
      "accuracy is 0.9676\n",
      "accuracy is 0.9697\n",
      "accuracy is 0.9656\n",
      "accuracy is 0.9677\n",
      "accuracy is 0.9704\n",
      "accuracy is 0.9715\n",
      "accuracy is 0.9695\n",
      "accuracy is 0.9694\n",
      "accuracy is 0.9706\n",
      "accuracy is 0.9721\n"
     ]
    }
   ],
   "source": [
    "    data = MNIST('./python-mnist/data')\n",
    "    training_images, training_labels = data.load_training()\n",
    "    test_images, test_labels = data.load_testing()\n",
    "\n",
    "    training_images = np.array(training_images)\n",
    "    test_images = np.array(test_images)\n",
    "    training_labels = np.array(training_labels)\n",
    "    test_labels = np.array(test_labels)\n",
    "\n",
    "\n",
    "    training_images = training_images / 127.5 - 1\n",
    "    test_images = test_images / 127.5 - 1\n",
    "\n",
    "    classes = 10\n",
    "\n",
    "    one_hot_train_labels = np.eye(classes)[training_labels] \n",
    "    one_hot_test_labels = np.eye(classes)[test_labels]  \n",
    "\n",
    "    nn = Network([784, 64, 10])\n",
    "\n",
    "    nn.train(training_images, one_hot_train_labels, training_labels, test_images, one_hot_test_labels, test_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 0.14829941 -0.01115394]\n",
      " [ 0.12750762  0.25495727]]\n"
     ]
    }
   ],
   "source": [
    "print(np.array(np.random.normal(0,np.power(5,-0.5),(2,2))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 2.89450221 -1.45678875]\n",
      " [ 0.41060642  0.50661766]]\n"
     ]
    }
   ],
   "source": [
    "print(np.array(np.random.randn(2, 2)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'dw' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-41-e9af3525258a>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdw\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m: name 'dw' is not defined"
     ]
    }
   ],
   "source": [
    "print(dw[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.0\n"
     ]
    }
   ],
   "source": [
    "print(1.2*np.array(0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
